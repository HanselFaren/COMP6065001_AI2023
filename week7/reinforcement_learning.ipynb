{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf1eada-d40b-491d-bb7a-6d92a4ce3ea2",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Outline\n",
    "1. Path finding example\n",
    "2. Getting started with Gym\n",
    "3. Designing AI agent\n",
    "4. Exercise\n",
    "5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45dd62c-d689-4876-ade9-00f4997c9e24",
   "metadata": {},
   "source": [
    "### 1. Path finding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066e507-89dc-4e0b-ae88-5111397c66ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from locale import currency\n",
    "import numpy as np\n",
    "\n",
    "R = np.matrix([[-1,-1,-1,-1,0,-1],\n",
    "               [-1,-1,-1,0,-1,100],\n",
    "               [-1,-1,-1,0,-1,-1],\n",
    "               [-1,0,0,-1,0,-1],\n",
    "               [-1,0,0,-1,-1,100],\n",
    "               [-1,0,-1,-1,0,100]])\n",
    "\n",
    "Q=np.matrix(np.zeros([6,6]))\n",
    "\n",
    "alpha = 0.8\n",
    "\n",
    "initial_state=1\n",
    "\n",
    "def available_actions(state):\n",
    "    curr_state_row = R[state,]\n",
    "    av_act = np.where(curr_state_row>=0)[1]\n",
    "    return av_act\n",
    "\n",
    "available_act = available_actions(initial_state)\n",
    "\n",
    "def sample_next_action(available_actions_range):\n",
    "    next_action = int(np.random.choice(available_act))\n",
    "    return next_action\n",
    "\n",
    "action = sample_next_action(available_act)\n",
    "\n",
    "def update (current_state, action, alpha):\n",
    "    max_index = np.where(Q[action,]==np.max(Q[action,]))[1]\n",
    "\n",
    "    if (max_index.shape[0] > 1):\n",
    "        max_index = int (np.random.choice(max_index))\n",
    "    else:\n",
    "        max_index = max_index[0]\n",
    "    max_value = Q[action, max_index]\n",
    "\n",
    "    #Q learning formula\n",
    "    Q[current_state,action] = R[current_state, action] + alpha*max_value\n",
    "\n",
    "update(initial_state, action, alpha)\n",
    "\n",
    "# TRAINING\n",
    "for i in range (10000):\n",
    "    current_state = np.random.randint(0,int(Q.shape[0]))\n",
    "    available_act = available_actions(current_state)\n",
    "    action = sample_next_action(available_act)\n",
    "    update(current_state, action, alpha)\n",
    "\n",
    "print(\"Trained Q matrix\")\n",
    "print(Q/np.max(Q)* 100)\n",
    "\n",
    "# TESTING\n",
    "goal_state = 5\n",
    "current_state = 2\n",
    "steps = [current_state]\n",
    "\n",
    "while current_state != goal_state:\n",
    "    next_step_index = np.where(Q[current_state,]==np.max(Q[current_state,]))[1]\n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index=int(np.random.choice(next_step_index))\n",
    "    else:\n",
    "        next_step_index = next_step_index[0]\n",
    "    steps.append(next_step_index)\n",
    "    current_state  =  next_step_index\n",
    "\n",
    "print(\"Selected path\")\n",
    "print(steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f01b29bc-2f3a-4388-8ff1-d5593615c34a",
   "metadata": {},
   "source": [
    "### 2. Getting started with Gym\n",
    "\n",
    "In this part, we install the necessary packages and <br>\n",
    "1) create an environment <br>\n",
    "2) draw the current state (observation) <br>\n",
    "3) create a default loop to interact with the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e086479-3ba8-403a-b38a-be3de07e0fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the library\n",
    "!pip install gymnasium\n",
    "!pip install pygame\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53c405-bfe8-434d-8606-d267e1da60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "\n",
    "# Observation and action space \n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f5ed8-10bb-43f0-a1ed-81124d11a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment and see the initial observation\n",
    "obs, info = env.reset()\n",
    "print(\"The initial observation is {}\".format(obs))\n",
    "\n",
    "# Draw the current state (observation)\n",
    "env_screen = env.render()\n",
    "env.close()\n",
    "\n",
    "plt.imshow(env_screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e33f16-fad6-4357-8039-cefce299d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main (default) loop of the simulation\n",
    "for _ in range(300):\n",
    "    # get sample action from action space\n",
    "    action = env.action_space.sample() \n",
    "\n",
    "    # apply selected action \n",
    "    # get new observation and reward\n",
    "    # status if terminated or truncated, and other debug info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbde283-a3df-41c3-9c34-b3f4990fa26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the current state (observation)\n",
    "env_screen = env.render()\n",
    "env.close()\n",
    "\n",
    "plt.imshow(env_screen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33efb9-3a42-46c8-bace-dbf340452b92",
   "metadata": {},
   "source": [
    "### 3. Designing AI Agent\n",
    "\n",
    "In this part, we create a class for our Agent that is not learning anything yet, <br>\n",
    "but selects his actions randomly given the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610dc75b-110a-49c2-a613-71e1441b497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.action_size = env.action_space.n\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        # select a random action\n",
    "        action = random.choice(range(self.action_size))\n",
    "        \n",
    "        # extract a pole angle from state\n",
    "        pole_angle = state[2]\n",
    "\n",
    "        # select action based on the pole angle\n",
    "        # 0 - Push cart to the left \n",
    "        # 1 - Push cart to the right\n",
    "        action = 0 if pole_angle < 0 else 1 \n",
    "        return action\n",
    "\n",
    "agent = Agent(env)\n",
    "state, _ = env.reset()\n",
    "print(\"Reset state: \", state)\n",
    "\n",
    "for i in range(100):\n",
    "    action = agent.get_action(state)\n",
    "    state, reward, done, trunc, info = env.step(action)\n",
    "    print(\"State at %d:\" % i, state)\n",
    "    print(reward,state,done)\n",
    "\n",
    "    if done:\n",
    "        env.reset()\n",
    "        env.close()\n",
    "        break\n",
    "    \n",
    "    env.render()\n",
    "    if reward<1:\n",
    "        sleep(3)\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad044f26-7d8d-457e-ac48-5013ad8ce0d0",
   "metadata": {},
   "source": [
    "### 4. Exercise\n",
    "Implement Q-learning for CartPole task in Gym.<br>\n",
    "\n",
    "There is a sample implementation (but uses outdated packages): <br>\n",
    "https://github.com/nnqomariyah/aima-python/blob/master/gym-cartpole-qlearning.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45905ebd-a56a-4677-8750-2f46175a766d",
   "metadata": {},
   "source": [
    "### 5. References\n",
    "\n",
    "1. Slides for week 7 - Reinforcement Learning\n",
    "2. https://github.com/aimacode/aima-python\n",
    "3. https://github.com/nnqomariyah/aima-python (forked)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a244e0b-05c2-4ef4-b7a8-b4a233f185fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
